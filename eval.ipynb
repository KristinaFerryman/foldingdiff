{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, '/net/dali/home/mscbio/jih323/foldingdiff_seq/foldingdiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/dali/home/mscbio/jih323/miniconda3/envs/foldingdiff/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from foldingdiff import datasets\n",
    "from foldingdiff import modelling\n",
    "from foldingdiff import beta_schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model dir\n",
    "# model_res_dir =\"./results_2/\"\n",
    "model_res_dir =\"./results_model1/\"\n",
    "Cache_name=\"./foldingdiff/cache_canonical_structures_cath_83a14642fa5c514f3bc80c553b5c801e.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "## read training configs\n",
    "params = {}\n",
    "with open(os.path.join(model_res_dir, 'training_args.json'), 'r') as f:\n",
    "    model_params = json.load(f)\n",
    "params.update(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import numpy as np\n",
    "\n",
    "ANGLES_DEFINITIONS = Literal[\n",
    "    \"canonical\" , \"canonical-full-angles\", \"canonical-minimal-angles\", \"cart-coords\"\n",
    "]\n",
    "\n",
    "def get_test_dsets(dataset_key: str = \"cath\",\n",
    "    angles_definitions: ANGLES_DEFINITIONS = \"canonical-full-angles\",\n",
    "    max_seq_len: int = 512,\n",
    "    min_seq_len: int = 0,\n",
    "    seq_trim_strategy: datasets.TRIM_STRATEGIES = \"leftalign\",\n",
    "    timesteps: int = 250,\n",
    "    variance_schedule: beta_schedules.SCHEDULES = \"linear\",\n",
    "    var_scale: float = np.pi,\n",
    "    exhaustive_t: bool = False,\n",
    "    splits=\"test\",\n",
    "    cache_fn=\"\"\n",
    "):\n",
    "    \n",
    "    clean_dset = datasets.CathCanonicalAnglesSequenceDataset(\n",
    "            pdbs=dataset_key,\n",
    "            split=splits,\n",
    "            pad=max_seq_len,\n",
    "            min_length=min_seq_len,\n",
    "            trim_strategy=seq_trim_strategy,\n",
    "            zero_center=False if angles_definitions == \"cart-coords\" else True,\n",
    "            use_cache=True,\n",
    "            cache_fn=cache_fn\n",
    "        )\n",
    "    \n",
    "    noised_dsets = datasets.NoisedAnglesDataset(\n",
    "            dset=clean_dset,\n",
    "            # dset_key=\"coords\" if angles_definitions == \"cart-coords\" else \"angles\",\n",
    "            dset_key=\"coords\" if angles_definitions == \"cart-coords\" else [\"angles\",\"sequence\"],    ## load both angles and sequence\n",
    "            timesteps=timesteps,\n",
    "            exhaustive_t=exhaustive_t,\n",
    "            beta_schedule=variance_schedule,\n",
    "            nonangular_variance=1.0,\n",
    "            angular_variance=var_scale,\n",
    "        )\n",
    "\n",
    "    for dsname, ds in zip([splits], [noised_dsets]):\n",
    "        print(f\"{dsname}: {ds}\")\n",
    "    \n",
    "    return noised_dsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache file ./foldingdiff/cache_canonical_structures_cath_83a14642fa5c514f3bc80c553b5c801e.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Mismatched hashes between codebase and cached values; updating cached values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: NoisedAnglesDataset wrapping <foldingdiff.datasets.CathCanonicalAnglesSequenceDataset object at 0x7fc4d4094460> with 3210 examples with cosine-1000 with variance scales 1.0 and 1.0\n"
     ]
    }
   ],
   "source": [
    "dsets = get_test_dsets(\n",
    "        dataset_key=params['dataset_key'],\n",
    "        angles_definitions=params['angles_definitions'],\n",
    "        max_seq_len=params['max_seq_len'],\n",
    "        min_seq_len=params['min_seq_len'],\n",
    "        seq_trim_strategy=params['trim_strategy'],\n",
    "        # seq_trim_strategy='discard',\n",
    "        timesteps=params['timesteps'],\n",
    "        variance_schedule=params['variance_schedule'],\n",
    "        var_scale=params['variance_scale'],\n",
    "        exhaustive_t=params[\"exhaustive_validation_t\"],\n",
    "        splits=\"test\",\n",
    "        cache_fn=Cache_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_model(dset, model, device):\n",
    "    # if torch.cuda.is_available():\n",
    "    #     effective_batch_size = int(batch_size / torch.cuda.device_count())\n",
    "    test_dataloader = DataLoader(\n",
    "            dataset=dset,\n",
    "            batch_size = params['batch_size'],\n",
    "            shuffle=False,  # Shuffle only train loader\n",
    "            num_workers=1,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "    '''\n",
    "    batch:\n",
    "    angles torch.Size([64, 128, 6])\n",
    "    coords torch.Size([64, 128, 3])\n",
    "    sequence torch.Size([64, 128])\n",
    "    attn_mask torch.Size([64, 128])\n",
    "    position_ids torch.Size([64, 128])\n",
    "    lengths torch.Size([64])\n",
    "    corrupted torch.Size([64, 128, 7])\n",
    "    t torch.Size([64, 1])\n",
    "    known_noise torch.Size([64, 128, 7])\n",
    "    sqrt_alphas_cumprod_t torch.Size([64])\n",
    "    sqrt_one_minus_alphas_cumprod_t torch.Size([64])\n",
    "    '''\n",
    "    input_seqs = []\n",
    "    denoised_values = []\n",
    "\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        predicted_noise = model(\n",
    "            batch[\"corrupted\"].to(device),\n",
    "            batch[\"t\"].to(device),\n",
    "            attention_mask=batch[\"attn_mask\"].to(device),\n",
    "            position_ids=batch[\"position_ids\"].to(device),\n",
    "        ).detach().cpu()\n",
    "        bs = batch[\"sqrt_one_minus_alphas_cumprod_t\"].shape[0]\n",
    "        denoised_angles = (\n",
    "            batch[\"corrupted\"]\n",
    "            - batch[\"sqrt_one_minus_alphas_cumprod_t\"].view(bs, 1, 1)\n",
    "            * predicted_noise\n",
    "        )\n",
    "        denoised_angles /= batch[\"sqrt_alphas_cumprod_t\"].view(bs, 1, 1)\n",
    "        # denoised_values.append(denoised_angles[:,:,6])\n",
    "        denoised_values.extend([denoised_angles[i,:,6][batch[\"attn_mask\"][i].bool()] for i in range(bs)])\n",
    "        # input_seqs.append(batch['sequence'][batch[\"attn_mask\"].bool()])\n",
    "        input_seqs.extend([batch['sequence'][i][batch[\"attn_mask\"][i].bool()] for i in range(bs)])\n",
    "    \n",
    "    # return torch.cat(denoised_values), torch.cat(input_seqs)\n",
    "    return denoised_values, input_seqs\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using time embedding: GaussianFourierProjection()\n"
     ]
    }
   ],
   "source": [
    "device=torch.device(\"cuda:1\")\n",
    "model = modelling.BertForDiffusionBase.from_dir(\n",
    "    model_res_dir\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForDiffusionBase(\n",
       "  (inputs_to_hidden_dim): Linear(in_features=7, out_features=512, bias=True)\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (distance_embedding): Embedding(255, 32)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (distance_embedding): Embedding(255, 32)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (distance_embedding): Embedding(255, 32)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (distance_embedding): Embedding(255, 32)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (distance_embedding): Embedding(255, 32)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (distance_embedding): Embedding(255, 32)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (distance_embedding): Embedding(255, 32)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (distance_embedding): Embedding(255, 32)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (distance_embedding): Embedding(255, 32)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (distance_embedding): Embedding(255, 32)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (distance_embedding): Embedding(255, 32)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (distance_embedding): Embedding(255, 32)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_decoder): AnglesPredictor(\n",
       "    (dense1): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    (dense2): Linear(in_features=512, out_features=7, bias=True)\n",
       "  )\n",
       "  (time_embed): GaussianFourierProjection()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:10<00:00,  4.73it/s]\n"
     ]
    }
   ],
   "source": [
    "denoised_values, input_seqs = test_model(dsets, model=model, device=device)\n",
    "# input_seqs = test_model(dsets, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2751, -0.5176, -2.8245,  0.5538,  2.6222, -1.4976,  2.6102, -2.2824,\n",
       "        -0.1829, -2.0967])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoised_values[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1571,  0.1571, -2.9845,  0.1571,  2.6704, -1.0996,  1.7279, -2.6704,\n",
       "        -0.4712, -2.0420])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seqs[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_order = ['T', 'S', 'A', 'G', 'P', 'R', 'K', 'N', 'D', 'E', 'Q', 'H', 'Y', 'W', 'F', 'L', 'M', 'I', 'V', 'C']\n",
    "seq2angle = {aa:angle for aa, angle in zip(seq_order, (np.linspace(-np.pi, np.pi, 20, endpoint=False)+np.pi/20))}\n",
    "angle2seq = {angle:aa for aa, angle in seq2angle.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_aa(preds: torch.Tensor, angle2seq: dict) -> list:\n",
    "    label_keys = torch.tensor(list(angle2seq.keys()))  # tensor of label keys\n",
    "    assigned_labels = []\n",
    "    \n",
    "    for v in preds:\n",
    "        distances = torch.abs(label_keys - v)  # find absolute distance to each label\n",
    "        closest_idx = torch.argmin(distances)  # find the index of the closest label\n",
    "        closest_label = angle2seq[label_keys[closest_idx].item()]  # retrieve label\n",
    "        assigned_labels.append(closest_label)\n",
    "    \n",
    "    return assigned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R', 'D', 'S', 'H', 'V', 'R', 'V', 'A', 'E', 'G']\n",
      "['Q', 'Q', 'T', 'Q', 'V', 'K', 'L', 'S', 'D', 'G']\n"
     ]
    }
   ],
   "source": [
    "print(classify_aa(denoised_values[0][:10], angle2seq))\n",
    "print(classify_aa(input_seqs[0][:10], angle2seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_pred_acc(input_seqs, denoised_values):\n",
    "    # bs = input_seqs.shape[0]\n",
    "    bs = len(input_seqs)\n",
    "    acc_all = []\n",
    "    for i in range(bs):\n",
    "        labels_input = classify_aa(input_seqs[i], angle2seq)\n",
    "        labels_output = classify_aa(denoised_values[i], angle2seq)\n",
    "        acc = sum(a == b for a, b in zip(labels_input, labels_output))/input_seqs[i].shape[-1]\n",
    "        acc_all.append(acc)\n",
    "    return acc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_all = seq_pred_acc(input_seqs, denoised_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2734375,\n",
       " 0.09166666666666666,\n",
       " 0.0546875,\n",
       " 0.28125,\n",
       " 0.5581395348837209,\n",
       " 0.6,\n",
       " 0.0945945945945946,\n",
       " 0.2578125,\n",
       " 0.2882882882882883,\n",
       " 0.9765625,\n",
       " 0.09375,\n",
       " 0.011235955056179775,\n",
       " 0.1640625,\n",
       " 0.0625,\n",
       " 0.09375,\n",
       " 0.2734375,\n",
       " 0.47619047619047616,\n",
       " 0.2112676056338028,\n",
       " 0.425531914893617,\n",
       " 0.1953125,\n",
       " 0.171875,\n",
       " 0.09375,\n",
       " 0.1015625,\n",
       " 0.2653061224489796,\n",
       " 0.2265625,\n",
       " 0.10144927536231885,\n",
       " 0.1875,\n",
       " 0.08333333333333333,\n",
       " 0.0821917808219178,\n",
       " 0.125,\n",
       " 0.265625,\n",
       " 0.1328125,\n",
       " 0.19827586206896552,\n",
       " 0.1171875,\n",
       " 0.0625,\n",
       " 0.046875,\n",
       " 0.9609375,\n",
       " 0.16455696202531644,\n",
       " 0.125,\n",
       " 1.0,\n",
       " 0.4945054945054945,\n",
       " 0.07352941176470588,\n",
       " 0.25961538461538464,\n",
       " 0.0390625,\n",
       " 0.5390625,\n",
       " 0.2734375,\n",
       " 0.1796875,\n",
       " 1.0,\n",
       " 0.0546875,\n",
       " 0.0390625,\n",
       " 0.18253968253968253,\n",
       " 0.2109375,\n",
       " 0.1,\n",
       " 0.0375,\n",
       " 0.5377358490566038,\n",
       " 0.0625,\n",
       " 0.06349206349206349,\n",
       " 0.1484375,\n",
       " 0.0761904761904762,\n",
       " 0.18032786885245902,\n",
       " 0.1111111111111111,\n",
       " 0.15625,\n",
       " 0.14285714285714285,\n",
       " 0.2542372881355932,\n",
       " 0.1328125,\n",
       " 0.3359375,\n",
       " 0.015625,\n",
       " 0.296875,\n",
       " 1.0,\n",
       " 0.11904761904761904,\n",
       " 0.2625,\n",
       " 0.96875,\n",
       " 0.359375,\n",
       " 0.0546875,\n",
       " 0.2265625,\n",
       " 0.0390625,\n",
       " 0.45454545454545453,\n",
       " 0.1015625,\n",
       " 0.4583333333333333,\n",
       " 0.11904761904761904,\n",
       " 0.16666666666666666,\n",
       " 0.08928571428571429,\n",
       " 0.03773584905660377,\n",
       " 0.015625,\n",
       " 0.10909090909090909,\n",
       " 0.0390625,\n",
       " 0.0703125,\n",
       " 0.0761904761904762,\n",
       " 0.578125,\n",
       " 0.421875,\n",
       " 0.40540540540540543,\n",
       " 0.3939393939393939,\n",
       " 0.10227272727272728,\n",
       " 0.5234375,\n",
       " 0.140625,\n",
       " 0.078125,\n",
       " 0.078125,\n",
       " 0.2578125,\n",
       " 0.19298245614035087,\n",
       " 0.7109375,\n",
       " 0.41,\n",
       " 0.203125,\n",
       " 0.17307692307692307,\n",
       " 0.37,\n",
       " 1.0,\n",
       " 0.5546875,\n",
       " 0.1308411214953271,\n",
       " 0.046511627906976744,\n",
       " 0.515625,\n",
       " 0.6171875,\n",
       " 0.265625,\n",
       " 0.05042016806722689,\n",
       " 1.0,\n",
       " 0.3828125,\n",
       " 0.1328125,\n",
       " 0.1015625,\n",
       " 0.0234375,\n",
       " 0.171875,\n",
       " 0.984375,\n",
       " 0.296875,\n",
       " 0.425531914893617,\n",
       " 0.11363636363636363,\n",
       " 0.056179775280898875,\n",
       " 0.3203125,\n",
       " 0.2421875,\n",
       " 0.7105263157894737,\n",
       " 0.390625,\n",
       " 0.2127659574468085,\n",
       " 0.26666666666666666,\n",
       " 0.4423076923076923,\n",
       " 0.046875,\n",
       " 0.19469026548672566,\n",
       " 0.9830508474576272,\n",
       " 0.8828125,\n",
       " 0.171875,\n",
       " 0.064,\n",
       " 1.0,\n",
       " 0.3302752293577982,\n",
       " 0.0859375,\n",
       " 0.5078125,\n",
       " 0.46875,\n",
       " 0.027777777777777776,\n",
       " 0.5078125,\n",
       " 0.5234375,\n",
       " 0.4166666666666667,\n",
       " 0.140625,\n",
       " 0.12195121951219512,\n",
       " 1.0,\n",
       " 0.1328125,\n",
       " 1.0,\n",
       " 0.71875,\n",
       " 0.2265625,\n",
       " 0.5703125,\n",
       " 0.06818181818181818,\n",
       " 0.0546875,\n",
       " 0.3671875,\n",
       " 0.10743801652892562,\n",
       " 0.21875,\n",
       " 0.171875,\n",
       " 0.7288135593220338,\n",
       " 0.671875,\n",
       " 0.046875,\n",
       " 0.11382113821138211,\n",
       " 0.4342105263157895,\n",
       " 0.6875,\n",
       " 0.3,\n",
       " 0.1875,\n",
       " 0.2421875,\n",
       " 0.1953125,\n",
       " 0.0546875,\n",
       " 0.109375,\n",
       " 0.20512820512820512,\n",
       " 0.0703125,\n",
       " 0.9008264462809917,\n",
       " 0.18681318681318682,\n",
       " 0.234375,\n",
       " 0.0847457627118644,\n",
       " 0.6146788990825688,\n",
       " 0.140625,\n",
       " 0.109375,\n",
       " 0.08181818181818182,\n",
       " 0.0703125,\n",
       " 0.14492753623188406,\n",
       " 0.109375,\n",
       " 0.1640625,\n",
       " 0.078125,\n",
       " 0.0625,\n",
       " 0.34375,\n",
       " 0.6818181818181818,\n",
       " 0.1875,\n",
       " 0.08771929824561403,\n",
       " 0.09,\n",
       " 0.07692307692307693,\n",
       " 0.0703125,\n",
       " 0.17355371900826447,\n",
       " 0.09174311926605505,\n",
       " 0.265625,\n",
       " 0.9453125,\n",
       " 0.078125,\n",
       " 0.2109375,\n",
       " 0.125,\n",
       " 0.4296875,\n",
       " 0.3669724770642202,\n",
       " 0.859375,\n",
       " 0.05982905982905983,\n",
       " 0.2265625,\n",
       " 0.34375,\n",
       " 0.265625,\n",
       " 0.08130081300813008,\n",
       " 0.08333333333333333,\n",
       " 0.46875,\n",
       " 0.967479674796748,\n",
       " 1.0,\n",
       " 0.3434343434343434,\n",
       " 0.3125,\n",
       " 0.6015625,\n",
       " 0.06779661016949153,\n",
       " 0.05511811023622047,\n",
       " 0.875,\n",
       " 0.15625,\n",
       " 0.13513513513513514,\n",
       " 0.45535714285714285,\n",
       " 0.19318181818181818,\n",
       " 0.1640625,\n",
       " 0.1484375,\n",
       " 0.4074074074074074,\n",
       " 0.15625,\n",
       " 0.30327868852459017,\n",
       " 0.5,\n",
       " 0.5671641791044776,\n",
       " 0.40625,\n",
       " 0.1485148514851485,\n",
       " 0.203125,\n",
       " 0.639344262295082,\n",
       " 0.07216494845360824,\n",
       " 0.171875,\n",
       " 0.6484375,\n",
       " 0.01694915254237288,\n",
       " 0.27586206896551724,\n",
       " 0.0625,\n",
       " 0.0703125,\n",
       " 0.3228346456692913,\n",
       " 0.5339805825242718,\n",
       " 0.6484375,\n",
       " 0.11864406779661017,\n",
       " 0.40625,\n",
       " 0.056179775280898875,\n",
       " 0.25287356321839083,\n",
       " 0.078125,\n",
       " 0.7837837837837838,\n",
       " 0.32098765432098764,\n",
       " 0.28169014084507044,\n",
       " 0.24050632911392406,\n",
       " 0.3883495145631068,\n",
       " 0.125,\n",
       " 0.9453125,\n",
       " 0.078125,\n",
       " 0.17307692307692307,\n",
       " 1.0,\n",
       " 0.3984375,\n",
       " 0.1015625,\n",
       " 0.12790697674418605,\n",
       " 0.058823529411764705,\n",
       " 0.0703125,\n",
       " 0.9921875,\n",
       " 0.4444444444444444,\n",
       " 0.9609375,\n",
       " 0.1953125,\n",
       " 0.15625,\n",
       " 0.0625,\n",
       " 0.16326530612244897,\n",
       " 0.359375,\n",
       " 0.0234375,\n",
       " 0.352112676056338,\n",
       " 0.25,\n",
       " 0.28125,\n",
       " 0.1328125,\n",
       " 0.4666666666666667,\n",
       " 0.0625,\n",
       " 0.125,\n",
       " 0.890625,\n",
       " 0.140625,\n",
       " 0.0234375,\n",
       " 0.046875,\n",
       " 0.1015625,\n",
       " 0.3984375,\n",
       " 0.4296875,\n",
       " 0.0859375,\n",
       " 0.1328125,\n",
       " 0.05555555555555555,\n",
       " 0.6692913385826772,\n",
       " 0.4765625,\n",
       " 0.046875,\n",
       " 0.1566265060240964,\n",
       " 0.359375,\n",
       " 0.09333333333333334,\n",
       " 0.4296875,\n",
       " 0.046875,\n",
       " 0.04040404040404041,\n",
       " 0.3645833333333333,\n",
       " 0.2734375,\n",
       " 0.6171875,\n",
       " 0.5,\n",
       " 0.140625,\n",
       " 0.0234375,\n",
       " 0.4375,\n",
       " 0.0703125,\n",
       " 0.328125,\n",
       " 0.07920792079207921,\n",
       " 0.19402985074626866,\n",
       " 0.15584415584415584,\n",
       " 0.7708333333333334,\n",
       " 0.1328125,\n",
       " 0.109375,\n",
       " 0.5546875,\n",
       " 0.05747126436781609,\n",
       " 0.07142857142857142,\n",
       " 0.2734375,\n",
       " 0.10112359550561797,\n",
       " 0.1875,\n",
       " 0.109375,\n",
       " 0.06172839506172839,\n",
       " 0.10869565217391304,\n",
       " 0.0546875,\n",
       " 0.265625,\n",
       " 1.0,\n",
       " 0.0546875,\n",
       " 0.0625,\n",
       " 0.046875,\n",
       " 0.03125,\n",
       " 0.3046875,\n",
       " 1.0,\n",
       " 0.171875,\n",
       " 0.05970149253731343,\n",
       " 0.043478260869565216,\n",
       " 0.0703125,\n",
       " 0.109375,\n",
       " 0.2807017543859649,\n",
       " 0.1328125,\n",
       " 0.676923076923077,\n",
       " 0.9888888888888889,\n",
       " 0.23809523809523808,\n",
       " 0.03225806451612903,\n",
       " 0.265625,\n",
       " 0.03409090909090909,\n",
       " 0.296875,\n",
       " 0.8359375,\n",
       " 1.0,\n",
       " 0.02127659574468085,\n",
       " 0.1015625,\n",
       " 0.9814814814814815,\n",
       " 0.05128205128205128,\n",
       " 0.14545454545454545,\n",
       " 0.03125,\n",
       " 0.0859375,\n",
       " 0.0703125,\n",
       " 0.19736842105263158,\n",
       " 0.5645161290322581,\n",
       " 0.03125,\n",
       " 0.2,\n",
       " 0.6557377049180327,\n",
       " 0.1171875,\n",
       " 0.056910569105691054,\n",
       " 0.5119047619047619,\n",
       " 0.07608695652173914,\n",
       " 0.3559322033898305,\n",
       " 0.3046875,\n",
       " 0.7090909090909091,\n",
       " 0.2890625,\n",
       " 0.1919191919191919,\n",
       " 0.0625,\n",
       " 0.04938271604938271,\n",
       " 0.453125,\n",
       " 0.078125,\n",
       " 0.04716981132075472,\n",
       " 0.0625,\n",
       " 0.058823529411764705,\n",
       " 0.53125,\n",
       " 0.2972972972972973,\n",
       " 0.1328125,\n",
       " 0.06930693069306931,\n",
       " 0.09782608695652174,\n",
       " 0.171875,\n",
       " 0.2976190476190476,\n",
       " 0.5703125,\n",
       " 0.09782608695652174,\n",
       " 0.0703125,\n",
       " 0.3828125,\n",
       " 0.0625,\n",
       " 0.43617021276595747,\n",
       " 0.046875,\n",
       " 0.07777777777777778,\n",
       " 0.34375,\n",
       " 0.8571428571428571,\n",
       " 0.15625,\n",
       " 0.8984375,\n",
       " 0.09375,\n",
       " 0.1328125,\n",
       " 0.031746031746031744,\n",
       " 0.7844827586206896,\n",
       " 0.4921875,\n",
       " 0.046875,\n",
       " 0.3515625,\n",
       " 0.0390625,\n",
       " 0.9821428571428571,\n",
       " 0.6,\n",
       " 0.65625,\n",
       " 0.0703125,\n",
       " 0.640625,\n",
       " 0.15625,\n",
       " 0.13924050632911392,\n",
       " 0.21100917431192662,\n",
       " 0.015151515151515152,\n",
       " 0.3828125,\n",
       " 0.05128205128205128,\n",
       " 0.359375,\n",
       " 0.11764705882352941,\n",
       " 0.2109375,\n",
       " 0.046875,\n",
       " 0.7333333333333333,\n",
       " 0.4296875,\n",
       " 1.0,\n",
       " 0.25,\n",
       " 0.0449438202247191,\n",
       " 0.0078125,\n",
       " 0.022222222222222223,\n",
       " 0.0625,\n",
       " 0.27472527472527475,\n",
       " 0.203125,\n",
       " 0.4375,\n",
       " 0.046875,\n",
       " 0.078125,\n",
       " 0.30612244897959184,\n",
       " 0.06349206349206349,\n",
       " 0.06349206349206349,\n",
       " 0.1414141414141414,\n",
       " 0.328125,\n",
       " 0.4609375,\n",
       " 0.8403361344537815,\n",
       " 0.9876543209876543,\n",
       " 0.10526315789473684,\n",
       " 0.1015625,\n",
       " 0.1328125,\n",
       " 1.0,\n",
       " 0.10752688172043011,\n",
       " 0.1953125,\n",
       " 0.5234375,\n",
       " 0.546875,\n",
       " 0.07563025210084033,\n",
       " 0.08450704225352113,\n",
       " 1.0,\n",
       " 0.058823529411764705,\n",
       " 0.03488372093023256,\n",
       " 0.1015625,\n",
       " 0.1875,\n",
       " 0.1875,\n",
       " 0.1875,\n",
       " 0.96875,\n",
       " 0.13043478260869565,\n",
       " 0.390625,\n",
       " 0.2777777777777778,\n",
       " 0.421875,\n",
       " 0.3203125,\n",
       " 0.052083333333333336,\n",
       " 0.1640625,\n",
       " 0.4140625,\n",
       " 0.171875,\n",
       " 0.265625,\n",
       " 1.0,\n",
       " 0.109375,\n",
       " 0.054945054945054944,\n",
       " 0.06422018348623854,\n",
       " 0.1171875,\n",
       " 0.21428571428571427,\n",
       " 0.0703125,\n",
       " 0.494949494949495,\n",
       " 0.38,\n",
       " 0.703125,\n",
       " 0.1095890410958904,\n",
       " 0.07766990291262135,\n",
       " 0.03125,\n",
       " 0.21311475409836064,\n",
       " 0.171875,\n",
       " 0.04395604395604396,\n",
       " 0.984375,\n",
       " 0.359375,\n",
       " 0.07317073170731707,\n",
       " 0.0380952380952381,\n",
       " 0.078125,\n",
       " 0.3125,\n",
       " 0.3359375,\n",
       " 0.0703125,\n",
       " 0.1015625,\n",
       " 0.15384615384615385,\n",
       " 0.0703125,\n",
       " 0.225,\n",
       " 0.0703125,\n",
       " 0.18181818181818182,\n",
       " 0.21739130434782608,\n",
       " 0.09803921568627451,\n",
       " 0.16666666666666666,\n",
       " 0.0859375,\n",
       " 0.0390625,\n",
       " 0.25742574257425743,\n",
       " 0.1640625,\n",
       " 0.0703125,\n",
       " 0.0859375,\n",
       " 0.6171875,\n",
       " 0.1171875,\n",
       " 0.3046875,\n",
       " 0.8582677165354331,\n",
       " 0.0859375,\n",
       " 0.046875,\n",
       " 0.1171875,\n",
       " 0.078125,\n",
       " 0.2578125,\n",
       " 0.31746031746031744,\n",
       " 0.7164179104477612,\n",
       " 0.7890625,\n",
       " 0.1640625,\n",
       " 0.09375,\n",
       " 0.48484848484848486,\n",
       " 0.0859375,\n",
       " 0.1953125,\n",
       " 0.078125,\n",
       " 0.6875,\n",
       " 0.06666666666666667,\n",
       " 0.359375,\n",
       " 0.09375,\n",
       " 0.1276595744680851,\n",
       " 0.2641509433962264,\n",
       " 0.8671875,\n",
       " 0.16279069767441862,\n",
       " 0.16901408450704225,\n",
       " 0.0234375,\n",
       " 0.10112359550561797,\n",
       " 1.0,\n",
       " 0.46875,\n",
       " 0.5855855855855856,\n",
       " 0.390625,\n",
       " 0.046875,\n",
       " 0.11904761904761904,\n",
       " 0.8070175438596491,\n",
       " 0.0859375,\n",
       " 0.1414141414141414,\n",
       " 0.109375,\n",
       " 0.041666666666666664,\n",
       " 0.6767676767676768,\n",
       " 0.5390625,\n",
       " 0.4482758620689655,\n",
       " 0.3157894736842105,\n",
       " 0.828125,\n",
       " 0.1953125,\n",
       " 0.15,\n",
       " 0.13333333333333333,\n",
       " 0.1328125,\n",
       " 0.0234375,\n",
       " 0.296875,\n",
       " 0.046511627906976744,\n",
       " 0.296875,\n",
       " 0.07142857142857142,\n",
       " 0.0546875,\n",
       " 0.22077922077922077,\n",
       " 0.32051282051282054,\n",
       " 0.125,\n",
       " 0.09375,\n",
       " 0.25,\n",
       " 0.515625,\n",
       " 0.125,\n",
       " 0.5859375,\n",
       " 0.17355371900826447,\n",
       " 0.0703125,\n",
       " 0.2727272727272727,\n",
       " 1.0,\n",
       " 0.2169811320754717,\n",
       " 0.125,\n",
       " 0.8790322580645161,\n",
       " 0.109375,\n",
       " 0.1953125,\n",
       " 0.3984375,\n",
       " 0.3046875,\n",
       " 0.3515625,\n",
       " 0.875,\n",
       " 0.421875,\n",
       " 0.05511811023622047,\n",
       " 0.5631067961165048,\n",
       " 0.18421052631578946,\n",
       " 0.05128205128205128,\n",
       " 0.1171875,\n",
       " 0.0859375,\n",
       " 0.703125,\n",
       " 0.1328125,\n",
       " 0.546875,\n",
       " 0.8984375,\n",
       " 0.569620253164557,\n",
       " 0.1171875,\n",
       " 0.1388888888888889,\n",
       " 0.2265625,\n",
       " 0.578125,\n",
       " 0.0625,\n",
       " 0.2543859649122807,\n",
       " 0.09210526315789473,\n",
       " 0.2265625,\n",
       " 0.6015625,\n",
       " 0.09375,\n",
       " 0.39473684210526316,\n",
       " 0.171875,\n",
       " 0.2265625,\n",
       " 0.6929133858267716,\n",
       " 0.25,\n",
       " 0.021739130434782608,\n",
       " 0.1015625,\n",
       " 0.171875,\n",
       " 0.041666666666666664,\n",
       " 0.046875,\n",
       " 0.6033057851239669,\n",
       " 0.2421875,\n",
       " 0.22105263157894736,\n",
       " 0.1206896551724138,\n",
       " 0.053763440860215055,\n",
       " 0.618421052631579,\n",
       " 0.10714285714285714,\n",
       " 0.046875,\n",
       " 0.140625,\n",
       " 0.453125,\n",
       " 0.1111111111111111,\n",
       " 0.3984375,\n",
       " 0.3515625,\n",
       " 0.24561403508771928,\n",
       " 0.234375,\n",
       " 0.078125,\n",
       " 0.109375,\n",
       " 0.03389830508474576,\n",
       " 0.25,\n",
       " 0.02608695652173913,\n",
       " 0.0380952380952381,\n",
       " 0.4609375,\n",
       " 0.12727272727272726,\n",
       " 0.2109375,\n",
       " 0.9453125,\n",
       " 0.125,\n",
       " 0.1796875,\n",
       " 0.2578125,\n",
       " 0.2578125,\n",
       " 0.171875,\n",
       " 0.1511627906976744,\n",
       " 0.140625,\n",
       " 0.3203125,\n",
       " 0.021739130434782608,\n",
       " 0.6621621621621622,\n",
       " 0.3523809523809524,\n",
       " 0.0390625,\n",
       " 0.4574468085106383,\n",
       " 0.3968253968253968,\n",
       " 0.2734375,\n",
       " 0.1484375,\n",
       " 0.6953125,\n",
       " 0.11403508771929824,\n",
       " 0.15625,\n",
       " 0.07407407407407407,\n",
       " 0.34375,\n",
       " 0.59375,\n",
       " 0.24786324786324787,\n",
       " 0.175,\n",
       " 0.6363636363636364,\n",
       " 0.4296875,\n",
       " 0.984375,\n",
       " 0.4166666666666667,\n",
       " 0.7734375,\n",
       " 0.2459016393442623,\n",
       " 0.5777777777777777,\n",
       " 0.2421875,\n",
       " 0.1640625,\n",
       " 1.0,\n",
       " 0.421875,\n",
       " 0.01694915254237288,\n",
       " 0.20175438596491227,\n",
       " 0.025,\n",
       " 0.08620689655172414,\n",
       " 0.3787878787878788,\n",
       " 0.09375,\n",
       " 0.28125,\n",
       " 0.045454545454545456,\n",
       " 0.390625,\n",
       " 0.11267605633802817,\n",
       " 0.3305785123966942,\n",
       " 0.0625,\n",
       " 0.0546875,\n",
       " 0.0625,\n",
       " 0.08139534883720931,\n",
       " 0.0234375,\n",
       " 0.0078125,\n",
       " 0.2109375,\n",
       " 0.609375,\n",
       " 0.12121212121212122,\n",
       " 0.2421875,\n",
       " 0.78125,\n",
       " 0.0703125,\n",
       " 0.140625,\n",
       " 0.09375,\n",
       " 0.546875,\n",
       " 0.06930693069306931,\n",
       " 0.0625,\n",
       " 0.390625,\n",
       " 0.19642857142857142,\n",
       " 0.765625,\n",
       " 0.15625,\n",
       " 0.1111111111111111,\n",
       " 0.3984375,\n",
       " 0.028985507246376812,\n",
       " 0.6551724137931034,\n",
       " 0.1640625,\n",
       " 0.686046511627907,\n",
       " 1.0,\n",
       " 0.18181818181818182,\n",
       " 0.078125,\n",
       " 0.09375,\n",
       " 0.5078125,\n",
       " 0.15625,\n",
       " 0.20754716981132076,\n",
       " 0.9230769230769231,\n",
       " 0.0546875,\n",
       " 0.30952380952380953,\n",
       " 0.2734375,\n",
       " 0.171875,\n",
       " 0.6396396396396397,\n",
       " 0.0703125,\n",
       " 0.039603960396039604,\n",
       " 0.328125,\n",
       " 0.10638297872340426,\n",
       " 0.1640625,\n",
       " 0.0703125,\n",
       " 0.078125,\n",
       " 0.256198347107438,\n",
       " 0.3828125,\n",
       " 0.078125,\n",
       " 0.109375,\n",
       " 0.1328125,\n",
       " 0.0859375,\n",
       " 0.2755102040816326,\n",
       " 0.49056603773584906,\n",
       " 0.30275229357798167,\n",
       " 0.4296875,\n",
       " 1.0,\n",
       " 0.3466666666666667,\n",
       " 0.9690721649484536,\n",
       " 0.23,\n",
       " 0.390625,\n",
       " 0.3333333333333333,\n",
       " 0.26666666666666666,\n",
       " 0.3984375,\n",
       " 0.1796875,\n",
       " 0.2421875,\n",
       " 0.0546875,\n",
       " 0.453125,\n",
       " 0.1328125,\n",
       " 0.14563106796116504,\n",
       " 0.5652173913043478,\n",
       " 0.0703125,\n",
       " 0.8448275862068966,\n",
       " 0.9921875,\n",
       " 1.0,\n",
       " 0.12931034482758622,\n",
       " 0.1484375,\n",
       " 0.12195121951219512,\n",
       " 0.3548387096774194,\n",
       " 0.6363636363636364,\n",
       " 0.5411764705882353,\n",
       " 0.7746478873239436,\n",
       " 0.3203125,\n",
       " 0.25,\n",
       " 0.4609375,\n",
       " 0.2265625,\n",
       " 0.546875,\n",
       " 0.0625,\n",
       " 0.0,\n",
       " 0.20212765957446807,\n",
       " 0.2421875,\n",
       " 0.1875,\n",
       " 0.024691358024691357,\n",
       " 0.140625,\n",
       " 0.265625,\n",
       " 0.0625,\n",
       " 0.25833333333333336,\n",
       " 0.7109375,\n",
       " 0.2578125,\n",
       " 0.2734375,\n",
       " 0.75,\n",
       " 0.140625,\n",
       " 0.4421052631578947,\n",
       " 0.1015625,\n",
       " 0.06956521739130435,\n",
       " 0.1484375,\n",
       " 0.25396825396825395,\n",
       " 0.16901408450704225,\n",
       " 0.23529411764705882,\n",
       " 0.2892561983471074,\n",
       " 0.1953125,\n",
       " 0.0703125,\n",
       " 0.28125,\n",
       " 0.6015625,\n",
       " 0.390625,\n",
       " 0.2421875,\n",
       " 0.296875,\n",
       " 0.640625,\n",
       " 0.0392156862745098,\n",
       " 0.3673469387755102,\n",
       " 0.09090909090909091,\n",
       " 0.1796875,\n",
       " 0.6962025316455697,\n",
       " 0.328125,\n",
       " 0.4166666666666667,\n",
       " 0.9921875,\n",
       " 0.022988505747126436,\n",
       " 0.6138613861386139,\n",
       " 0.2125,\n",
       " 0.375,\n",
       " 0.07017543859649122,\n",
       " 0.359375,\n",
       " 0.2890625,\n",
       " 0.10344827586206896,\n",
       " 0.6126126126126126,\n",
       " 0.14173228346456693,\n",
       " 0.0703125,\n",
       " 0.1015625,\n",
       " 0.4057971014492754,\n",
       " 0.07142857142857142,\n",
       " 0.0234375,\n",
       " 0.0859375,\n",
       " 0.0703125,\n",
       " 0.5859375,\n",
       " 0.025,\n",
       " 0.04,\n",
       " 0.5909090909090909,\n",
       " 0.9210526315789473,\n",
       " 0.02,\n",
       " 0.10638297872340426,\n",
       " 0.0625,\n",
       " 0.09090909090909091,\n",
       " 0.1328125,\n",
       " 0.6521739130434783,\n",
       " 0.373015873015873,\n",
       " 0.0625,\n",
       " 0.03125,\n",
       " 0.28125,\n",
       " 0.046875,\n",
       " 0.1171875,\n",
       " 0.2421875,\n",
       " 0.0390625,\n",
       " 0.046875,\n",
       " 0.4523809523809524,\n",
       " 0.09375,\n",
       " 0.08943089430894309,\n",
       " 0.373015873015873,\n",
       " 0.203125,\n",
       " 0.109375,\n",
       " 0.125,\n",
       " 0.07894736842105263,\n",
       " 0.10714285714285714,\n",
       " 0.8018867924528302,\n",
       " 0.0703125,\n",
       " 0.184,\n",
       " 0.28125,\n",
       " 0.1015625,\n",
       " 0.012195121951219513,\n",
       " 0.0625,\n",
       " 0.515625,\n",
       " 0.1328125,\n",
       " 0.0703125,\n",
       " 0.07246376811594203,\n",
       " 0.3116883116883117,\n",
       " 0.02,\n",
       " 0.171875,\n",
       " 0.15,\n",
       " 0.0547945205479452,\n",
       " 1.0,\n",
       " 0.1188118811881188,\n",
       " 0.1171875,\n",
       " 0.22826086956521738,\n",
       " 0.2890625,\n",
       " 0.09375,\n",
       " 0.40707964601769914,\n",
       " 0.46875,\n",
       " 0.234375,\n",
       " 0.078125,\n",
       " 0.078125,\n",
       " 0.6015625,\n",
       " 0.0859375,\n",
       " 0.7446808510638298,\n",
       " 0.5859375,\n",
       " 0.1875,\n",
       " 0.8545454545454545,\n",
       " 0.1484375,\n",
       " 0.234375,\n",
       " 0.08196721311475409,\n",
       " 0.9871794871794872,\n",
       " 0.2421875,\n",
       " 0.0390625,\n",
       " 0.2978723404255319,\n",
       " 0.9259259259259259,\n",
       " 0.27450980392156865,\n",
       " 0.05263157894736842,\n",
       " 0.225,\n",
       " 0.1896551724137931,\n",
       " 0.4140625,\n",
       " 0.109375,\n",
       " 0.06542056074766354,\n",
       " 0.7578125,\n",
       " 0.15853658536585366,\n",
       " 0.8271604938271605,\n",
       " 0.8076923076923077,\n",
       " 0.15625,\n",
       " 0.25,\n",
       " 0.417910447761194,\n",
       " 0.1111111111111111,\n",
       " 0.40625,\n",
       " 0.1015625,\n",
       " 0.9375,\n",
       " 0.5234375,\n",
       " 0.4765625,\n",
       " 0.017241379310344827,\n",
       " 0.046875,\n",
       " 0.041666666666666664,\n",
       " 0.140625,\n",
       " 0.4765625,\n",
       " 1.0,\n",
       " 0.375,\n",
       " 0.13725490196078433,\n",
       " 0.05555555555555555,\n",
       " 0.04807692307692308,\n",
       " 0.13924050632911392,\n",
       " 0.0625,\n",
       " 0.03125,\n",
       " 0.28125,\n",
       " 0.34579439252336447,\n",
       " 0.1796875,\n",
       " 0.3157894736842105,\n",
       " 0.09433962264150944,\n",
       " 0.2079207920792079,\n",
       " 0.15625,\n",
       " 0.3333333333333333,\n",
       " 0.1076923076923077,\n",
       " 0.40625,\n",
       " 0.0546875,\n",
       " 0.1484375,\n",
       " 0.046875,\n",
       " 0.5304347826086957,\n",
       " 0.6307692307692307,\n",
       " 0.0546875,\n",
       " 0.25,\n",
       " 0.0891089108910891,\n",
       " 0.609375,\n",
       " 0.5277777777777778,\n",
       " 0.06666666666666667,\n",
       " 0.5178571428571429,\n",
       " 1.0,\n",
       " 0.28125,\n",
       " 0.10714285714285714,\n",
       " 0.765625,\n",
       " 0.1328125,\n",
       " 0.056338028169014086,\n",
       " 0.8515625,\n",
       " 0.039603960396039604,\n",
       " 0.038461538461538464,\n",
       " 0.30952380952380953,\n",
       " 0.4264705882352941,\n",
       " 0.59375,\n",
       " 0.1015625,\n",
       " 0.1721311475409836,\n",
       " 0.1875,\n",
       " 0.2054794520547945,\n",
       " 0.0234375,\n",
       " 0.42857142857142855,\n",
       " 0.984375,\n",
       " 0.2839506172839506,\n",
       " 0.0703125,\n",
       " 0.546875,\n",
       " 0.09375,\n",
       " 0.04597701149425287,\n",
       " 0.49572649572649574,\n",
       " 0.078125,\n",
       " 0.09375,\n",
       " 0.140625,\n",
       " 0.15384615384615385,\n",
       " 0.046875,\n",
       " 0.16666666666666666,\n",
       " 0.1484375,\n",
       " 0.21875,\n",
       " 0.11607142857142858,\n",
       " 0.71875,\n",
       " 0.7553191489361702,\n",
       " 0.4140625,\n",
       " 0.515625,\n",
       " 0.0784313725490196,\n",
       " 0.25,\n",
       " 0.2265625,\n",
       " 0.1875,\n",
       " 0.11475409836065574,\n",
       " 0.96875,\n",
       " 0.1640625,\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28819297196876775"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(acc_all, bins=30)\n",
    "plt.xlabel('count')\n",
    "plt.ylabel('acc')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foldingdiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
